{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6 + Homework 3: MLPs + Dropout + CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much of this was taken from DS-GA 1011 course from last semester."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're doing a sentiment classification task. So first load the Stanford Sentiment Treebank data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import re\n",
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "sst_home = 'data/trees'\n",
    "\n",
    "# Let's do 2-way positive/negative classification instead of 5-way\n",
    "easy_label_map = {0:0, 1:0, 2:None, 3:1, 4:1}\n",
    "    # so labels of 0 and 1 in te 5-wayclassificaiton are 0 in the 2-way. 3 and 4 are 1, and 2 is none\n",
    "    # because we don't have a neautral class. \n",
    "\n",
    "PADDING = \"<PAD>\"\n",
    "UNKNOWN = \"<UNK>\"\n",
    "max_seq_length = 20\n",
    "\n",
    "def load_sst_data(path):\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for i, line in enumerate(f): \n",
    "            example = {}\n",
    "            example['label'] = easy_label_map[int(line[1])]\n",
    "            if example['label'] is None:\n",
    "                continue\n",
    "            \n",
    "            # Strip out the parse information and the phrase labels---we don't need those here\n",
    "            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n",
    "            example['text'] = text[1:]\n",
    "            data.append(example)\n",
    "\n",
    "    random.seed(1)\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "     \n",
    "training_set = load_sst_data(sst_home + '/train.txt')\n",
    "dev_set = load_sst_data(sst_home + '/dev.txt')\n",
    "test_set = load_sst_data(sst_home + '/test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And extract bag-of-words feature vectors. For speed, we'll only use words that appear at least 25 times in the training set, leaving us with |V|=1254."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "def tokenize(string):\n",
    "    return string.split()\n",
    "\n",
    "def build_dictionary(training_datasets):\n",
    "    \"\"\"\n",
    "    Extract vocabulary and build dictionary.\n",
    "    \"\"\"  \n",
    "    word_counter = collections.Counter()\n",
    "    for i, dataset in enumerate(training_datasets):\n",
    "        for example in dataset:\n",
    "            word_counter.update(tokenize(example['text']))\n",
    "        \n",
    "    vocabulary = set([word for word in word_counter])\n",
    "    vocabulary = list(vocabulary)\n",
    "    vocabulary = [PADDING, UNKNOWN] + vocabulary\n",
    "        \n",
    "    word_indices = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "\n",
    "    return word_indices, len(vocabulary)\n",
    "\n",
    "def sentences_to_padded_index_sequences(word_indices, datasets):\n",
    "    \"\"\"\n",
    "    Annotate datasets with feature vectors. Adding right-sided padding. \n",
    "    \"\"\"\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        for example in dataset:\n",
    "            example['text_index_sequence'] = torch.zeros(max_seq_length)\n",
    "\n",
    "            token_sequence = tokenize(example['text'])\n",
    "            padding = max_seq_length - len(token_sequence)\n",
    "\n",
    "            for i in range(max_seq_length):\n",
    "                if i >= len(token_sequence):\n",
    "                    index = word_indices[PADDING]\n",
    "                    pass\n",
    "                else:\n",
    "                    if token_sequence[i] in word_indices:\n",
    "                        index = word_indices[token_sequence[i]]\n",
    "                    else:\n",
    "                        index = word_indices[UNKNOWN]\n",
    "                example['text_index_sequence'][i] = index\n",
    "\n",
    "            example['text_index_sequence'] = example['text_index_sequence'].long().view(1,-1)\n",
    "            example['label'] = torch.LongTensor([example['label']])\n",
    "\n",
    "\n",
    "word_to_ix, vocab_size = build_dictionary([training_set])\n",
    "sentences_to_padded_index_sequences(word_to_ix, [training_set, dev_set, test_set])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to feed data to our model in mini-batches so we need a data iterator that will \"batchify\" the data. We "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the iterator we'll use during training. \n",
    "# It's a generator that gives you one batch at a time.\n",
    "def data_iter(source, batch_size):\n",
    "    dataset_size = len(source)\n",
    "    start = -1 * batch_size\n",
    "    order = list(range(dataset_size))\n",
    "    random.shuffle(order)\n",
    "\n",
    "    while True:\n",
    "        start += batch_size\n",
    "        if start > dataset_size - batch_size:\n",
    "            # Start another epoch.\n",
    "            start = 0\n",
    "            random.shuffle(order)   \n",
    "        batch_indices = order[start:start + batch_size]\n",
    "        yield [source[index] for index in batch_indices]\n",
    "\n",
    "# This is the iterator we use when we're evaluating our model. \n",
    "# It gives a list of batches that you can then iterate through.\n",
    "def eval_iter(source, batch_size):\n",
    "    batches = []\n",
    "    dataset_size = len(source)\n",
    "    start = -1 * batch_size\n",
    "    order = list(range(dataset_size))\n",
    "    random.shuffle(order)\n",
    "\n",
    "    while start < dataset_size - batch_size:\n",
    "        start += batch_size\n",
    "        batch_indices = order[start:start + batch_size]\n",
    "        batch = [source[index] for index in batch_indices]\n",
    "        batches.append(batch)\n",
    "        \n",
    "    return batches\n",
    "\n",
    "# The following function gives batches of vectors and labels, \n",
    "# these are the inputs to your model and loss function\n",
    "def get_batch(batch):\n",
    "    vectors = []\n",
    "    labels = []\n",
    "    for dict in batch:\n",
    "        vectors.append(dict[\"text_index_sequence\"])\n",
    "        labels.append(dict[\"label\"])\n",
    "    return vectors, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define an evaluation function,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data_iter):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(len(data_iter)):\n",
    "        vectors, labels = get_batch(data_iter[i])\n",
    "        vectors = Variable(torch.stack(vectors).squeeze())\n",
    "        labels = torch.stack(labels).squeeze()\n",
    "        output = model(vectors)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "    return correct / float(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a multilayer perceptron classifier.\n",
    "\n",
    "What hyperparameters do you think would work well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A Multi-Layer Perceptron (MLP)\n",
    "class MLPClassifier(nn.Module): # inheriting from nn.Module!\n",
    "    \n",
    "    def __init__(self, input_size, embedding_dim, hidden_dim, num_labels, dropout_prob):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(input_size, embedding_dim, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "            \n",
    "        self.linear_1 = nn.Linear(embedding_dim, hidden_dim) \n",
    "        self.linear_2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear_3 = nn.Linear(hidden_dim, num_labels)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input through your layers in order\n",
    "        out = self.embed(x)\n",
    "        out = self.dropout(out)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out = F.relu(self.linear_1(out))\n",
    "        out = F.relu(self.linear_2(out))\n",
    "        out = self.dropout(self.linear_3(out))\n",
    "        return out\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        lin_layers = [self.linear_1, self.linear_2]\n",
    "        em_layer = [self.embed]\n",
    "     \n",
    "        for layer in lin_layers+em_layer:\n",
    "            layer.weight.data.uniform_(-initrange, initrange)\n",
    "            if layer in lin_layers:\n",
    "                layer.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define our training loop,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_loop(model, loss, optimizer, training_iter, dev_iter, train_eval_iter):\n",
    "    step = 0\n",
    "    results = []\n",
    "    for i in range(num_train_steps):\n",
    "        model.train()\n",
    "        vectors, labels = get_batch(next(training_iter))\n",
    "        vectors = Variable(torch.stack(vectors).squeeze())\n",
    "        labels = Variable(torch.stack(labels).squeeze())\n",
    "\n",
    "        model.zero_grad()\n",
    "        output = model(vectors)\n",
    "\n",
    "        lossy = loss(output, labels)\n",
    "        lossy.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            \n",
    "            print( \"Step %i; Loss %f; Train acc: %f; Dev acc %f\" \n",
    "                %(step, lossy.data[0], evaluate(model, train_eval_iter), evaluate(model, dev_iter)))\n",
    "\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper Parameters \n",
    "input_size = vocab_size\n",
    "num_labels = 2\n",
    "batch_size = 32\n",
    "num_train_steps = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify these hyperparameters to try to achieve approximately 80% dev accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_dim = 40\n",
    "embedding_dim = 300\n",
    "learning_rate = 0.001\n",
    "dropout_prob = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can build and train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0; Loss 0.689027; Train acc: 0.480000; Dev acc 0.474000\n",
      "Step 100; Loss 0.612122; Train acc: 0.704000; Dev acc 0.676000\n",
      "Step 200; Loss 0.464455; Train acc: 0.882000; Dev acc 0.748000\n",
      "Step 300; Loss 0.599127; Train acc: 0.906000; Dev acc 0.760000\n",
      "Step 400; Loss 0.308638; Train acc: 0.918000; Dev acc 0.768000\n",
      "Step 500; Loss 0.213843; Train acc: 0.968000; Dev acc 0.772000\n",
      "Step 600; Loss 0.323684; Train acc: 0.974000; Dev acc 0.786000\n",
      "Step 700; Loss 0.081667; Train acc: 0.976000; Dev acc 0.770000\n",
      "Step 800; Loss 0.054736; Train acc: 0.988000; Dev acc 0.786000\n",
      "Step 900; Loss 0.137428; Train acc: 0.986000; Dev acc 0.766000\n"
     ]
    }
   ],
   "source": [
    "model = MLPClassifier(input_size, embedding_dim, hidden_dim, num_labels, dropout_prob)\n",
    "    \n",
    "# Loss and Optimizer\n",
    "loss = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "training_iter = data_iter(training_set, batch_size)\n",
    "train_eval_iter = eval_iter(training_set[0:500], batch_size)\n",
    "dev_iter = eval_iter(dev_set[0:500], batch_size)\n",
    "training_loop(model, loss, optimizer, training_iter, dev_iter, train_eval_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it performs on the held out test set,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the CBOW on the test data: 0.742449\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "test_iter = eval_iter(test_set, batch_size)\n",
    "test_acc = evaluate(model, test_iter)\n",
    "print('Accuracy of the CBOW on the test data: %f' % (test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "This model does okay. It doesn't do that well. Lets try and define a Convolutional Neural Network to try and improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, input_size, embedding_dim, window_size, n_filters, num_labels, dropout_prob):\n",
    "        super(TextCNN, self).__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(input_size, embedding_dim, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(p = dropout_prob)\n",
    "        self.dropout2 = nn.Dropout(p = dropout_prob)\n",
    "        self.conv1 = nn.Conv2d(1, n_filters, (window_size, embedding_dim)) \n",
    "        self.fc1 = nn.Linear(n_filters, num_labels)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input through your layers in order\n",
    "        out = self.embed(x)\n",
    "        out = self.dropout(out)\n",
    "        out = out.unsqueeze(1)\n",
    "        out = self.conv1(out).squeeze(3)\n",
    "        out = F.relu(out)\n",
    "        out = F.max_pool1d(out, out.size(2)).squeeze(2)\n",
    "        out = self.fc1(self.dropout2(out))\n",
    "        return out\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        lin_layers = [self.fc1]\n",
    "        em_layer = [self.embed]\n",
    "     \n",
    "        for layer in lin_layers+em_layer:\n",
    "            layer.weight.data.uniform_(-initrange, initrange)\n",
    "            if layer in lin_layers:\n",
    "                layer.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets train our Conv Net. Lets redefine the hyperparameters here. You need to modify these as well! Try to achieve approximately 80% dev accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "n_filters = 10\n",
    "embedding_dim = 90\n",
    "learning_rate = 0.001\n",
    "dropout_prob = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets build and train this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0; Loss 0.696573; Train acc: 0.464000; Dev acc 0.460000\n",
      "Step 100; Loss 0.692624; Train acc: 0.618000; Dev acc 0.580000\n",
      "Step 200; Loss 0.622002; Train acc: 0.760000; Dev acc 0.694000\n",
      "Step 300; Loss 0.510301; Train acc: 0.842000; Dev acc 0.758000\n",
      "Step 400; Loss 0.446919; Train acc: 0.910000; Dev acc 0.788000\n",
      "Step 500; Loss 0.183383; Train acc: 0.948000; Dev acc 0.786000\n",
      "Step 600; Loss 0.159861; Train acc: 0.962000; Dev acc 0.764000\n",
      "Step 700; Loss 0.038178; Train acc: 0.974000; Dev acc 0.772000\n",
      "Step 800; Loss 0.241119; Train acc: 0.982000; Dev acc 0.772000\n",
      "Step 900; Loss 0.046731; Train acc: 0.990000; Dev acc 0.778000\n"
     ]
    }
   ],
   "source": [
    "cnn_model = TextCNN(input_size, embedding_dim, window_size, n_filters, num_labels, dropout_prob)\n",
    "    \n",
    "# Loss and Optimizer\n",
    "loss = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(cnn_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "training_iter = data_iter(training_set, batch_size)\n",
    "train_eval_iter = eval_iter(training_set[0:500], batch_size)\n",
    "dev_iter = eval_iter(dev_set[0:500], batch_size)\n",
    "training_loop(cnn_model, loss, optimizer, training_iter, dev_iter, train_eval_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets evaluate this on the held out test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the CNN model on the test data: 0.741900\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "test_iter = eval_iter(test_set, batch_size)\n",
    "test_acc = evaluate(cnn_model, test_iter)\n",
    "print('Accuracy of the CNN model on the test data: %f' % (test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 (10pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please construct all of your plots in the ipython notebook using something like matplotlib. Provide all answers in the ipython notebook. We will not grade anything other than the ipython notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "\n",
    "1. Provide plots of varying hidden_dim, embedding_dim, LR, and dropout for deep CBOW (0.75pts each). \n",
    "2. Describe how each hyperparameter affects performance on train and dev (1.5pts total).\n",
    "3. Provide plots of varying embedding_dim, window_size, num_filters, LR, and dropout for CNN (0.6pts for each HP).\n",
    "4. Describe how each hyperparameter affects performance on train and dev (1.5pts total).\n",
    "5. Write down an hyperparameter configuration for CBOW that achieves 80 dev within the first 1000 train steps. Make sure this configuration is run in your ipython notebook when it is submitted (0.5pts).\n",
    "6. Write down an hyperparameter configuration for CNN that achieves 80 dev within the first 1000 train steps. Make sure this configuration is run in your ipython notebook when it is submitted (0.5pts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
